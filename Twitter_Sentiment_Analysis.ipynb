{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16owuaqEnQpe"
      },
      "outputs": [],
      "source": [
        "#Description: This is a sentiment analysis program that parses the tweets fetched from Twitter using Python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Libraries\n",
        "# Install Libraries\n",
        "!pip install textblob\n",
        "!pip install tweepy"
      ],
      "metadata": {
        "id": "pO-20vrHpjKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the libraries\n",
        "from textblob import TextBlob\n",
        "import sys\n",
        "import tweepy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "plt.style.use('fivethirtyeight')"
      ],
      "metadata": {
        "id": "ah6nTC3MA6gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "yutcec8CA9m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the data\n",
        "log = pd.read_csv('Login.csv')"
      ],
      "metadata": {
        "id": "cZCfpssbBDln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Twitter API Credentials\n",
        "consumerKey = log['key'][0]\n",
        "consumerSecret = log['key'][1]\n",
        "accessToken = log['key'][2]\n",
        "accessTokenSecret = log['key'][3]"
      ],
      "metadata": {
        "id": "PMo_sYmCB728"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the authentication object\n",
        "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
        "\n",
        "# Set the access token and access token secret\n",
        "auth.set_access_token(accessToken, accessTokenSecret)\n",
        "\n",
        "# Create the API object while passing in the auth information\n",
        "api = tweepy.API(auth, wait_on_rate_limit = True)"
      ],
      "metadata": {
        "id": "HgRmRytkCZJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweet_sentiment(self, tweet):\n",
        "  '''\n",
        "  Utility function to classify sentiment of passed tweet\n",
        "  using textblob's sentiment method\n",
        "  '''\n",
        "  # create TextBlob object of passed tweet text\n",
        "  analysis = TextBlob(self.clean_tweet(tweet))\n",
        "  # set sentiment\n",
        "  if analysis.sentiment.polarity > 0:\n",
        "      return 'positive'\n",
        "  elif analysis.sentiment.polarity == 0:\n",
        "      return 'neutral'\n",
        "  else:\n",
        "      return 'negative'"
      ],
      "metadata": {
        "id": "9XXn9PK70Pa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def percentage(part,whole):\n",
        " return 100 * float(part)/float(whole)\n",
        "\n",
        "keyword = input(\"lockdown Canada\")\n",
        "noOfTweet = int(input(\"2500\"))\n",
        "tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
        "positive = 0\n",
        "negative = 0\n",
        "neutral = 0\n",
        "polarity = 0\n",
        "tweet_list = []\n",
        "neutral_list = []\n",
        "negative_list = []\n",
        "positive_list = []\n",
        "for tweet in tweets:\n",
        " \n",
        " #print(tweet.text)\n",
        " tweet_list.append(tweet.text)\n",
        " analysis = TextBlob(tweet.text)\n",
        " score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
        " neg = score['neg']\n",
        " neu = score['neu']\n",
        " pos = score['pos']\n",
        " comp = score['compound']\n",
        " polarity += analysis.sentiment.polarity\n",
        "\n",
        " if neg > pos:\n",
        "   negative_list.append(tweet.text)\n",
        "   negative += 1\n",
        "\n",
        " elif pos > neg:\n",
        "   positive_list.append(tweet.text)\n",
        "   positive += 1\n",
        " \n",
        " elif pos == neg:\n",
        "   neutral_list.append(tweet.text)\n",
        "   neutral += 1\n",
        "\n",
        "positive = percentage(positive, noOfTweet)\n",
        "negative = percentage(negative, noOfTweet)\n",
        "neutral = percentage(neutral, noOfTweet)\n",
        "polarity = percentage(polarity, noOfTweet)\n",
        "positive = format(positive, '.1f')\n",
        "negative = format(negative, '.1f')\n",
        "neutral = format(neutral,'.1f')\n"
      ],
      "metadata": {
        "id": "e_RLrxRiA2vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
        "tweet_list = pd.DataFrame(tweet_list)\n",
        "neutral_list = pd.DataFrame(neutral_list)\n",
        "negative_list = pd.DataFrame(negative_list)\n",
        "positive_list = pd.DataFrame(positive_list)\n",
        "print(\"total number: \",len(tweet_list))\n",
        "print(\"positive number: \",len(positive_list))\n",
        "print(\"negative number: \", len(negative_list))\n",
        "print(\"neutral number: \",len(neutral_list))"
      ],
      "metadata": {
        "id": "yLc8c0rnwlO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating PieCart\n",
        "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
        "sizes = [positive, neutral, negative]\n",
        "colors = ['yellowgreen', 'blue','red']\n",
        "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
        "plt.style.use('default')\n",
        "plt.legend(labels)\n",
        "plt.title(\"Sentiment Analysis Result for keyword= \"+keyword+\"\" )\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nXQaWEX7wwRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet.list_drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "S1KVn9Lg2WHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning Text (RT, Punctutation etc)\n",
        "\n",
        "# Creating new dataframe and new features\n",
        "tw_list = pd.DataFrame(tweet_list)\n",
        "tw_list[\"text\"] = tw_lst[0]\n",
        "\n",
        "#Removing RT, Punctutation etc\n",
        "remove_rt = lambda x: re.sub('RT @\\w+:', \"\", x)\n",
        "rt = lambda x: re.sub(\"(@[A-Za-z0–9]+)|([⁰-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \"\", x)\n",
        "tw_list[\"text\"]\n",
        "tw_list.text.map(remove_rt).map(rt)\n",
        "tw_list[\"text\"] = tw_list.text.str.lower()\n",
        "tw_list.head(10)"
      ],
      "metadata": {
        "id": "G9RG3e182b6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Negative, Positive, Neutral, and Compound Values\n",
        "\n",
        "tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n",
        "for index, row in tw_list['text'].iteritems():\n",
        "  score = SentimentIntensityAnalyzer().polarity_scores(row)\n",
        "  neg = score['neg']\n",
        "  neu = score['neu']\n",
        "  pos = score['pos']\n",
        "  comp = score['compound']\n",
        "\n",
        "  if neg > pos:\n",
        "    tw_list.loc[index, 'sentiment'] = \"negative\"\n",
        "  elif pos > neg:\n",
        "    tw_list.loc[index, 'sentiment'] = \"positive\"\n",
        "  else:\n",
        "    tw_list.loc[index, 'sentiment'] = \"neutral\"\n",
        "    tw_list.loc[index, 'neg'] = neg\n",
        "    tw_list.loc[index, 'neu'] = neu\n",
        "    tw_list.loc[index, 'pos'] = pos\n",
        "    tw_list.loc[index, 'compound'] = comp\n",
        "\n",
        "tw_list.head(10)\n",
        "  "
      ],
      "metadata": {
        "id": "L2JT8M-p3nBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new data frames for all sentiments (positive, negative and neutral)\n",
        "\n",
        "tw_list_negative = \n",
        "tw_list[tw_list[\"sentiment\"]==\"negative\"]\n",
        "tw_list_positive = \n",
        "tw_list[tw_list[\"sentiment\"]==\"positive\"]\n",
        "tw_list_neutral = \n",
        "tw_list[tw_list[\"sentiment\"]==\"neutral\"]"
      ],
      "metadata": {
        "id": "LAu6z_Gk30tr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_values_in_column(data, feature):\n",
        "\n",
        "  total=data.loc[:,feature].value_counts(dropna=False)\n",
        "  percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n",
        "\n",
        "  return pd.concat([total,percentage],axis=1, keys=['Total','Percentage'])\n",
        "\n",
        "#Count_values for sentiment\n",
        "count_values_in_column(tw_list,\"sentiment\")"
      ],
      "metadata": {
        "id": "lHaZIfsU6DBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data for Pie Chart\n",
        "pichart = count_values_in_column(tw_list,\"sentiment\")\n",
        "names = pc.index\n",
        "size = pc[\"Percentage\"]\n",
        " \n",
        "# Create a circle for the center of the plot\n",
        "my_circle=plt.Circle( (0,0), 0.7, color='white')\n",
        "plt.pie(size, labels=names, colors=['green','blue','red'])\n",
        "p=plt.gcf()\n",
        "p.gca().add_artist(my_circle)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4bU-p10n63X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to Create Wordcloud\n",
        "\n",
        "def create_wordcloud(text):\n",
        "  mask = np.array(Image.open(\"cloud.png\"))\n",
        "  stopwords = set(STOPWORDS)\n",
        "  wc = WordCloud(background_color = \"white\", mask = mask, max_words = 3000, stopwords = stopwords, repeat = True)\n",
        "  wc.generate(str(text))\n",
        "  wc.to_file(\"wc.png\")\n",
        "  print(\"Word Cloud Saved Successfully\")\n",
        "  path = \"wc.png\"\n",
        "  display(Image.open(path))"
      ],
      "metadata": {
        "id": "lImDNNxF7e4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating wordcloud for all tweets\n",
        "create_wordcloud(tw_list[\"text\"].values)"
      ],
      "metadata": {
        "id": "RhJImGHr7xAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating wordcloud for positive sentiment\n",
        "create_wordcloud(tw_list_positive[\"text\"].values)"
      ],
      "metadata": {
        "id": "OOxVG0x68gK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating wordcloud for negative sentiment\n",
        "create_wordcloud(tw_list_negative[\"text\"].values)"
      ],
      "metadata": {
        "id": "-1xa630m8sBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating tweet's length and word count\n",
        "tw_list['text_len'] = tw_list['text'].astype(str).apply(len)\n",
        "tw_list['text_word_count'] = tw_list['text'].apply(lamb x: len(str(x).split()))\n",
        "\n",
        "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)\n",
        "round(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word.mean()),2)"
      ],
      "metadata": {
        "id": "EwnN8EGR82Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing Punctuation\n",
        "def remove_punct(text):\n",
        " text = \"\".join([char for char in text if char not in string.punctuation])\n",
        " text = re.sub('[0–9]+', '', text)\n",
        " return text\n",
        "\n",
        "tw_list['punct'] = tw_list['text'].apply(lambda x: remove_punct(x))"
      ],
      "metadata": {
        "id": "qJJoneGM9iPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Appliyng tokenization\n",
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "    \n",
        "tw_list['tokenized'] = tw_list['punct'].apply(lambda x: tokenization(x.lower()))"
      ],
      "metadata": {
        "id": "SL_37ccO9xVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing stopwords\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    return text\n",
        "    \n",
        "tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n"
      ],
      "metadata": {
        "id": "X0OJKB_f91IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Appliyng Stemmer\n",
        "ps = nltk.PorterStemmer()\n",
        "def stemming(text):\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))"
      ],
      "metadata": {
        "id": "P8cL48gf93bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Cleaning Text\n",
        "def clean_text(text):\n",
        "    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n",
        "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
        "    tokens = re.split('\\W+', text_rc)    # tokenization\n",
        "    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n",
        "    return text\n",
        "\n",
        "tw_list.head()"
      ],
      "metadata": {
        "id": "vVWViYoB961B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Appliyng Countvectorizer\n",
        "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
        "countVector = countVectorizer.fit_transform(tw_list['text'])\n",
        "print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n",
        "\n",
        "#print(countVectorizer.get_feature_names())\n",
        "count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n",
        "count_vect_df.head()"
      ],
      "metadata": {
        "id": "_t5aeZky-Bz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Used Words\n",
        "count = pd.DataFrame(count_vect_df.sum())\n",
        "countdf = count.sort_values(0,ascending=False).head(20)\n",
        "countdf[1:11]"
      ],
      "metadata": {
        "id": "qzsbxwWb-OZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to ngram\n",
        "def get_top_n_gram(corpus,ngram_range,n=None):\n",
        " vec = CountVectorizer(ngram_range=ngram_range,stop_words = 'english').fit(corpus)\n",
        " bag_of_words = vec.transform(corpus)\n",
        " sum_words = bag_of_words.sum(axis=0) \n",
        " words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        " words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        " return words_freq[:n]\n",
        "#n2_bigram\n",
        "n2_bigrams = get_top_n_gram(tw_list['text'],(2,2),20)\n",
        "n2_bigrams"
      ],
      "metadata": {
        "id": "36GvAzF7-S35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#n3_trigram\n",
        "n3_trigrams = get_top_n_gram(tw_list['text'],(3,3),20)\n",
        "n3_trigrams"
      ],
      "metadata": {
        "id": "4lSt-16e-Yyv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}